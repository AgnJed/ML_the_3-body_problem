{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Task 1: Setting the Baseline \n","## Task 1.1 Data Preparation and Validation Pipeline"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:26.348280Z","iopub.status.busy":"2024-10-16T11:11:26.347264Z","iopub.status.idle":"2024-10-16T11:11:30.655005Z","shell.execute_reply":"2024-10-16T11:11:30.654112Z","shell.execute_reply.started":"2024-10-16T11:11:26.348235Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","\n","df_raw_train = pd.read_csv(\"./kaggle/input/3bodyproblem/mlNOVA/mlNOVA/X_train.csv\")\n","df_raw_test = pd.read_csv(\"./kaggle/input/3bodyproblem/mlNOVA/mlNOVA/X_test.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### Check for any missing values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Summary of the missing\\nvalues in the training set\\n\")\n","print(df_raw_train.isna().sum())\n","print()\n","\n","print(\"Summary of the missing\\nvalues in the test set\\n\")\n","print(df_raw_test.isna().sum())"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize few trajectories"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:30.663583Z","iopub.status.busy":"2024-10-16T11:11:30.663083Z","iopub.status.idle":"2024-10-16T11:11:32.201363Z","shell.execute_reply":"2024-10-16T11:11:32.200272Z","shell.execute_reply.started":"2024-10-16T11:11:30.663546Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","start = 0\n","end = 257\n","\n","for i in range(5):\n","    trajectory = df_raw_train[start:end]\n","    x1, y1, x2, y2, x3, y3 = trajectory.x_1, trajectory.y_1, trajectory.x_2, trajectory.y_2, trajectory.x_3, trajectory.y_3\n","    plt.plot(x1, y1, color=\"hotpink\", label = \"Body 1\")\n","    plt.plot(x2, y2, color=\"blue\", label = \"Body 2\")\n","    plt.plot(x3, y3, color=\"purple\", label = \"Body 3\")\n","    plt.scatter(x1[start], y1[start], color=\"hotpink\")\n","    plt.scatter(x2[start], y2[start], color=\"blue\")\n","    plt.scatter(x3[start], y3[start], color=\"purple\")\n","    plt.xlabel(\"X\")\n","    plt.ylabel(\"Y\", rotation=0)\n","    plt.title(\"Trajectory no.\" + str(i+1))\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","    start += 257\n","    end += 257\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare the dataset"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.204842Z","iopub.status.busy":"2024-10-16T11:11:32.204119Z","iopub.status.idle":"2024-10-16T11:11:32.215174Z","shell.execute_reply":"2024-10-16T11:11:32.214197Z","shell.execute_reply.started":"2024-10-16T11:11:32.204793Z"},"trusted":true},"outputs":[],"source":["df_raw_train[\"group\"] = df_raw_train[\"Id\"]//257"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.216927Z","iopub.status.busy":"2024-10-16T11:11:32.216545Z","iopub.status.idle":"2024-10-16T11:11:32.243661Z","shell.execute_reply":"2024-10-16T11:11:32.242812Z","shell.execute_reply.started":"2024-10-16T11:11:32.216884Z"},"trusted":true},"outputs":[],"source":["#All starting positions\n","df_init = df_raw_train[ df_raw_train.index%257 == 0]"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.245176Z","iopub.status.busy":"2024-10-16T11:11:32.244812Z","iopub.status.idle":"2024-10-16T11:11:32.656324Z","shell.execute_reply":"2024-10-16T11:11:32.655168Z","shell.execute_reply.started":"2024-10-16T11:11:32.245140Z"},"trusted":true},"outputs":[],"source":["df_combined = df_raw_train.merge(df_init, on=\"group\",  suffixes=('', '_init'))\n","\n","# Step 1: Create a list/array of unique group_ids\n","unique_groups = df_combined['Id_init'].unique()\n","\n","# Step 2: Create a random mask for this list\n","mask_train_test = np.random.rand(len(unique_groups)) < 0.8  # random True/False values\n","\n","# Step 3: Split unique groups into two sets\n","group_train = unique_groups[mask_train_test]  # groups for the training set\n","group_testing = unique_groups[~mask_train_test]  # groups for the test set\n","\n","# Step 4: Use the mask to split the DataFrame\n","train_set = df_combined[df_combined['Id_init'].isin(group_train)]\n","testing_set = df_combined[df_combined['Id_init'].isin(group_testing)]\n","\n","\n","unique_groups_train_validation = testing_set['Id_init'].unique()\n","\n","mask_val_test = np.random.rand(len(unique_groups_train_validation)) < 0.5\n","\n","group_validation = unique_groups_train_validation[mask_val_test]  # groups for the training set\n","group_test = unique_groups_train_validation[~mask_val_test]  # groups for the test set\n","\n","validation_set = testing_set[testing_set['Id_init'].isin(group_validation)]\n","test_set = testing_set[testing_set['Id_init'].isin(group_test)]\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Remove following zeros after bodies collision (optional)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["# train_set = train_set.drop_duplicates(subset=[col for col in train_set.columns if col != 'id'])\n","# train_set.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### Display sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_set"]},{"cell_type":"markdown","metadata":{},"source":["### Verify dataset splitting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get sorted unique Id_init values\n","unique_sorted_ids_train = sorted(set(train_set['Id_init'].values))\n","unique_sorted_ids_val = sorted(set(validation_set['Id_init'].values))\n","unique_sorted_ids_test = sorted(set(test_set['Id_init'].values))\n","\n","# Find intersection of the two sets\n","intersection_ids1 = sorted(set(unique_sorted_ids_train) & set(unique_sorted_ids_val))\n","intersection_ids2 = sorted(set(unique_sorted_ids_train) & set(unique_sorted_ids_test))\n","intersection_ids3 = sorted(set(unique_sorted_ids_test) & set(unique_sorted_ids_val))\n","\n","# Display the results]\n","print(\"Intersection of IDs:\", intersection_ids1)\n","print(\"Intersection of IDs:\", intersection_ids2)\n","print(\"Intersection of IDs:\", intersection_ids3)"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare input and output sets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.657924Z","iopub.status.busy":"2024-10-16T11:11:32.657554Z","iopub.status.idle":"2024-10-16T11:11:32.690212Z","shell.execute_reply":"2024-10-16T11:11:32.689351Z","shell.execute_reply.started":"2024-10-16T11:11:32.657879Z"},"trusted":true},"outputs":[],"source":["input_train = train_set[[\"t\",\"x_1_init\", \"y_1_init\", \"x_2_init\", \"y_2_init\", \"x_3_init\", \"y_3_init\"]]\n","input_validation = validation_set[[\"t\",\"x_1_init\", \"y_1_init\", \"x_2_init\", \"y_2_init\", \"x_3_init\", \"y_3_init\"]]\n","input_test = test_set[[\"t\",\"x_1_init\", \"y_1_init\", \"x_2_init\", \"y_2_init\", \"x_3_init\", \"y_3_init\"]]\n","input_submission = df_raw_test.drop(columns=[\"Id\"]).rename(columns={\"t\": \"t\", \"x0_1\": \"x_1_init\", \"y0_1\": \"y_1_init\", \"x0_2\": \"x_2_init\", \"y0_2\": \"y_2_init\", \"x0_3\": \"x_3_init\", \"y0_3\": \"y_3_init\"})\n","input_submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_validation.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.691641Z","iopub.status.busy":"2024-10-16T11:11:32.691339Z","iopub.status.idle":"2024-10-16T11:11:32.722814Z","shell.execute_reply":"2024-10-16T11:11:32.721784Z","shell.execute_reply.started":"2024-10-16T11:11:32.691609Z"},"trusted":true},"outputs":[],"source":["output_train = train_set[[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n","output_validation = validation_set[[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n","output_test = test_set[[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n","\n","output_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_validation.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.725177Z","iopub.status.busy":"2024-10-16T11:11:32.724301Z","iopub.status.idle":"2024-10-16T11:11:32.733348Z","shell.execute_reply":"2024-10-16T11:11:32.732352Z","shell.execute_reply.started":"2024-10-16T11:11:32.725118Z"},"trusted":true},"outputs":[],"source":["groups = train_set[\"group\"]\n","groups"]},{"cell_type":"markdown","metadata":{},"source":["#### Function to save predictions to csv\n"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["def save_predictions_to_csv(predictions: np.ndarray, output_file_name: str):\n","\n","    \"\"\"\n","    Saves the given NumPy array with predictions to a CSV file.\n","\n","    Parameters:\n","    - predictions (np.ndarray): A NumPy array containing the data to be saved.\n","    - output_file_name (str): The name of the output CSV file (without .csv extension).\n","    \"\"\"\n","\n","    predictions_df = pd.DataFrame(predictions, columns=[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"])\n","    predictions_df.insert(0, 'Id', range(len(predictions_df)))\n","    predictions_df.to_csv(\"./output/csv/\"+output_file_name+\".csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Drop columns (optional)"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["# columns=[\"y_3_init\",\"x_3_init\"]\n","# test_set=test_set.drop(columns=columns)\n","# input_test=input_test.drop(columns=columns)\n","# input_validation=input_validation.drop(columns=columns)\n","# input_train=input_train.drop(columns=columns)\n","# input_submission=input_submission.drop(columns=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_validation.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Task 1.2 Learn the baseline model"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.737548Z","iopub.status.busy":"2024-10-16T11:11:32.736760Z","iopub.status.idle":"2024-10-16T11:11:32.743026Z","shell.execute_reply":"2024-10-16T11:11:32.742138Z","shell.execute_reply.started":"2024-10-16T11:11:32.737502Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","\n","nn = MLPRegressor(random_state=1, max_iter=3)\n","svm = SVR(kernel=\"linear\")\n","model = LinearRegression()\n","modelRidge = Ridge()\n","modelLasso = Lasso()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.744480Z","iopub.status.busy":"2024-10-16T11:11:32.744171Z","iopub.status.idle":"2024-10-16T11:11:32.756665Z","shell.execute_reply":"2024-10-16T11:11:32.755640Z","shell.execute_reply.started":"2024-10-16T11:11:32.744446Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","\n","pipe = Pipeline(\n","    [\n","            ('scaling', StandardScaler()), \n","            (\"model\", model)\n","    ])\n","pipe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T11:11:32.758300Z","iopub.status.busy":"2024-10-16T11:11:32.757930Z","iopub.status.idle":"2024-10-16T11:11:34.236735Z","shell.execute_reply":"2024-10-16T11:11:34.235228Z","shell.execute_reply.started":"2024-10-16T11:11:32.758267Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import root_mean_squared_error\n","    \n","pipe.fit(input_train, output_train)\n","y_hat = pipe.predict(input_validation)\n","y_hat_test = pipe.predict(input_test)\n","y_hat_submission = pipe.predict(input_submission)\n","save_predictions_to_csv(y_hat_test, \"baseline_test\")\n","save_predictions_to_csv(y_hat_submission, \"baseline-model\")\n","rmse = root_mean_squared_error(output_validation, y_hat)\n","rmse_test = root_mean_squared_error(output_test, y_hat_test)\n","\n","print(\"RMSE: \", rmse)\n","print(\"RMSE Test: \", rmse_test)"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.status.busy":"2024-10-16T11:11:34.237696Z","iopub.status.idle":"2024-10-16T11:11:34.238104Z","shell.execute_reply":"2024-10-16T11:11:34.237934Z","shell.execute_reply.started":"2024-10-16T11:11:34.237914Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def plot_y_yhat(y_validation, y_pred, plot_title = \"plot\"):\n","    labels = ['x_1','y_1','x_2','y_2','x_3','y_3']\n","    MAX = 500\n","    if len(y_validation) > MAX:\n","        idx = np.random.choice(len(y_validation),MAX, replace=False)\n","    else:\n","        idx = np.arange(len(y_validation))\n","    plt.figure(figsize=(10,10))\n","    for i in range(6):\n","        x0 = np.min(y_validation[idx,i])\n","        x1 = np.max(y_validation[idx,i])\n","        plt.subplot(3,2,i+1)\n","        plt.scatter(y_validation[idx,i],y_pred[idx,i])\n","        plt.xlabel('True '+labels[i])\n","        plt.ylabel('Predicted '+labels[i])\n","        plt.plot([x0,x1],[x0,x1],color='red')\n","        plt.axis('square')\n","    plt.suptitle(plot_title)\n","    plt.savefig(\"./output/plots/\"+plot_title+'.pdf')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-16T11:11:34.239477Z","iopub.status.idle":"2024-10-16T11:11:34.239851Z","shell.execute_reply":"2024-10-16T11:11:34.239688Z","shell.execute_reply.started":"2024-10-16T11:11:34.239670Z"},"trusted":true},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(y_hat).to_numpy(), plot_title=\"baseline\")"]},{"cell_type":"markdown","metadata":{},"source":["# Task 2: Nonlinear models on the data â€” the Polynomial Regression model"]},{"cell_type":"markdown","metadata":{},"source":["## Task 2.1 Development"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.status.busy":"2024-10-16T11:11:34.242662Z","iopub.status.idle":"2024-10-16T11:11:34.243074Z","shell.execute_reply":"2024-10-16T11:11:34.242900Z","shell.execute_reply.started":"2024-10-16T11:11:34.242856Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import RidgeCV\n","from sklearn.metrics import root_mean_squared_error\n","\n","def validate_poly_regression(X_train, y_train, X_val, y_val, regressor=None, degrees=range(1,9), max_features=None):\n","    #best_rsme = [1000, 1000, 1000]\n","    best_degrees_rsmes = [[0, 1000], [0, 1000], [0, 1000]]\n","    train_scores = []\n","    validation_scores = []\n","    n_features_list = []\n","\n","    for d in degrees:\n","        print(\"Degree \", d)\n","\n","        pipe_d_degree= Pipeline(\n","    [       ('preprocessor', PolynomialFeatures(degree=d)),\n","            ('model', RidgeCV(alphas=np.logspace(-6, 6, 13)))\n","    ])\n","\n","        pipe_d_degree.fit(X_train, y_train)\n","\n","        n_features = pipe_d_degree.named_steps['preprocessor'].n_output_features_\n","        n_features_list.append(n_features)\n","\n","        y_train_predict = pipe_d_degree.predict(X_train)\n","        y_val_predict = pipe_d_degree.predict(X_val)\n","\n","\n","        train_error = root_mean_squared_error(y_train, y_train_predict)\n","        #print(\"Train RMSE:\", train_error)\n","\n","        val_error = root_mean_squared_error(y_val, y_val_predict)\n","        print(\"Val RMSE:\", val_error)\n","\n","        if val_error < best_degrees_rsmes[2][1]:\n","            best_degrees_rsmes[2][1] = val_error\n","            best_degrees_rsmes[2][0] = d\n","            best_degrees_rsmes.sort(key=lambda x: x[1], reverse=False)\n","\n","        train_scores.append(train_error)\n","        validation_scores.append(val_error)\n","    \n","    best_degrees = [item[0] for item in best_degrees_rsmes]\n","    best_rsmes = [item[1] for item in best_degrees_rsmes]\n","\n","    return train_scores, validation_scores, best_rsmes, best_degrees, n_features_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["degrees=range(1,9)\n","degree_scores = {d: {'train_scores': [], 'test_scores': []} for d in degrees}\n","best_rsmes = []\n","best_degrees = []\n","\n","for _ in range(10):\n","\n","    sample_train_set = train_set.sample(frac=0.01)\n","    sample_input_train = sample_train_set[[\"t\",\"x_1_init\", \"y_1_init\", \"x_2_init\", \"y_2_init\", \"x_3_init\", \"y_3_init\"]]\n","    sample_output_train = sample_train_set[[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n","    \n","    train_scores, validation_scores, best_rsme, best_degree, n_features_list = validate_poly_regression(sample_input_train, sample_output_train, input_validation, output_validation, degrees=degrees)\n","\n","    best_rsmes.append(min(best_rsme))\n","    best_degrees.append(best_degree[0])\n","    best_degrees.append(best_degree[1])\n","    best_degrees.append(best_degree[2])\n","\n","    for d in degrees:\n","        degree_scores[d]['train_scores'].append(train_scores[d-1])\n","        degree_scores[d]['test_scores'].append(validation_scores[d-1])\n","\n","print(n_features_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Create bin edges so that each bar is centered on its respective integer\n","bin_edges = [i - 0.5 for i in range(1, len(degrees) + 2)]  # Create bin edges\n","\n","plt.hist(best_degrees, bins=bin_edges, color='pink', edgecolor='black')\n","\n","plt.xticks(range(1, len(degrees) + 1))\n","plt.yaxis.get_major_locator().set_params(integer=True)\n","\n","plt.title('Distribution of Selected Polynomial Degrees')\n","plt.xlabel('Degree')\n","plt.ylabel('Number of times in TOP3 degrees')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(best_rsmes, color='hotpink')\n","plt.title('RMSE throughout 10 runs')\n","plt.xlabel('# of run')\n","plt.ylabel('RMSE')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Train with the best-degree model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#best_degree_ever = max(set(best_degrees), key=best_degrees.count)\n","best_degree_ever=3\n","print(\"Best degree\", best_degree_ever)\n","\n","pipe_best_degree= Pipeline(\n","[       ('preprocessor', PolynomialFeatures(degree=best_degree_ever)),\n","        ('model', RidgeCV(alphas=np.logspace(-6, 6, 13)))\n","])\n","\n","pipe_best_degree.fit(input_train, output_train)\n","\n","output_train_predict = pipe_best_degree.predict(input_train)\n","output_val_predict = pipe_best_degree.predict(input_validation)\n","output_test_predict = pipe_best_degree.predict(input_test)\n","output_submission_predict = pipe_best_degree.predict(input_submission)\n","\n","save_predictions_to_csv(output_test_predict, \"poly_test\")\n","save_predictions_to_csv(output_submission_predict, \"polynomial_submission\")\n","\n","train_error = root_mean_squared_error(output_train, output_train_predict)\n","print(\"Train RMSE:\", train_error)\n","\n","val_error = root_mean_squared_error(output_validation, output_val_predict)\n","print(\"Validation RMSE:\", val_error)"]},{"cell_type":"markdown","metadata":{},"source":["### Train with the 3 best-degree models on more data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","\n","top_3_degrees = Counter(best_degrees).most_common(3)\n","\n","most_frequent_degrees = [degree for degree, count in top_3_degrees]\n","\n","print(\"Top 3 most frequent degrees:\", sorted(most_frequent_degrees))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["degrees=sorted(most_frequent_degrees)\n","degree_scores = {d: {'train_scores': [], 'test_scores': []} for d in degrees}\n","best_rsmes_after_selection = []\n","best_degrees_after_selection = []\n","\n","for _ in range(5):\n","\n","    sample_train_set = train_set.sample(frac=0.35)\n","    sample_input_train = sample_train_set[[\"t\",\"x_1_init\", \"y_1_init\", \"x_2_init\", \"y_2_init\", \"x_3_init\", \"y_3_init\"]]\n","    sample_output_train = sample_train_set[[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n","    \n","    train_scores, validation_scores, best_rsme, best_degree, n_features_list = validate_poly_regression(sample_input_train, sample_output_train, input_validation, output_validation, degrees=degrees)\n","\n","    best_index = best_rsme.index(min(best_rsme))\n","    best_rsmes_after_selection.append(best_rsme[best_index])\n","    best_degrees_after_selection.append(best_degree[best_index])\n","\n","    for d in degrees:\n","        degree_scores[d]['train_scores'].append(train_scores[d-1])\n","        degree_scores[d]['test_scores'].append(validation_scores[d-1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create bin edges so that each bar is centered on its respective integer\n","bin_edges = [i - 0.5 for i in range(1, len(degrees) + 2)]  # Create bin edges\n","\n","plt.hist(best_degrees_after_selection, bins=bin_edges, color='pink', edgecolor='black')\n","\n","plt.xticks(range(1, len(most_frequent_degrees) + 1))\n","plt.yaxis.get_major_locator().set_params(integer=True)\n","\n","plt.title('Distribution of Selected Polynomial Degrees')\n","plt.xlabel('Degree')\n","plt.ylabel('Frequency')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Train on the best-degree model on all data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_degree_ever = max(set(best_degrees), key=best_degrees.count)\n","\n","print(\"Best degree\", best_degree_ever)\n","\n","pipe_best_degree= Pipeline(\n","[       ('preprocessor', PolynomialFeatures(degree=best_degree_ever)),\n","        ('model', RidgeCV(alphas=np.logspace(-6, 6, 13)))\n","])\n","\n","pipe_best_degree.fit(input_train, output_train)\n","\n","output_train_predict = pipe_best_degree.predict(input_train)\n","output_val_predict = pipe_best_degree.predict(input_validation)\n","\n","train_error = root_mean_squared_error(output_train, output_train_predict)\n","print(\"Train RMSE:\", train_error)\n","\n","val_error = root_mean_squared_error(output_validation, output_val_predict)\n","print(\"Validation RMSE:\", val_error)"]},{"cell_type":"markdown","metadata":{},"source":["## Task 2.2 Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(output_val_predict).to_numpy(), plot_title=\"polynomial\")"]},{"cell_type":"markdown","metadata":{},"source":["# Task 3: Feature Engineering\n","## Task 3.1 Removing variables"]},{"cell_type":"markdown","metadata":{},"source":["### Seaborn plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","df_train=train_set[[\"t\",\"x_1\", \"v_x_1\", \"y_1\",\"v_y_1\", \"x_2\", \"v_x_2\", \"y_2\", \"v_y_2\", \"x_3\", \"v_x_3\", \"y_3\", \"v_y_3\"]]\n","sns.pairplot(df_train.sample(200), kind=\"hist\")"]},{"cell_type":"markdown","metadata":{},"source":["### Check linear corelations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corr = df_train.corr()\n","sns.heatmap(corr,annot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation map with absolute values\n","\n","sns.heatmap(corr.abs(),annot=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Tests for dropping columns and pair of columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import itertools\n","# List to store results\n","results = []\n","rmse_base=rmse\n","# Columns to consider for dropping\n","columns = input_train.columns\n","\n","# Iterate over combinations of 1, 2, and 3 columns\n","for num_columns in range(1, 4):\n","    for combo in itertools.combinations(columns, num_columns):\n","        modified_input = input_train.drop(columns=list(combo))\n","        pipe.fit(modified_input, output_train)\n","        y_hat = pipe.predict(input_validation.drop(columns=list(combo)))\n","        rmse = root_mean_squared_error(output_validation, y_hat)\n","        results.append((combo, rmse*10000))\n","\n","# Sort results by RMSE\n","sorted_results = sorted(results, key=lambda x: x[1])\n","\n","# Print sorted results\n","for combo, rmse in sorted_results:\n","    print(f\"Columns dropped: {combo}\")\n","    print(f\"RMSE: {(rmse-(rmse_base*10000))}\")\n","    print(\"-\" * 30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import root_mean_squared_error, mean_squared_error\n","from sklearn.model_selection import GroupKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","import math\n","\n","# Removing different variables\n","\n","model = LinearRegression()\n","pipe1 = Pipeline(\n","    [\n","            ('scaling', StandardScaler()), \n","            (\"model\", model)\n","    ])\n","\n","reduced_input=input_train[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\", \"y_3_init\"]]\n","reduced_val=input_validation[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\", \"y_3_init\"]]\n","\n","# Function to calculate RMSE\n","def calculate_rmse(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred, squared=False)\n","    return math.sqrt(mse)\n","\n","# Initialize baseline RMSE\n","pipe1.fit(reduced_input, output_train)\n","y_hat1 = pipe1.predict(reduced_val)\n","baseline_rmse = calculate_rmse(output_validation, y_hat1)\n","\n","print(\"Baseline RMSE: \", baseline_rmse)\n","\n","f_to_remove=[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\", \"y_3_init\"]\n","\n","# Loop to remove one variable at a time\n","for i in range(len(f_to_remove)):\n","    # Create a new list of features excluding the i-th feature\n","    reduced_features = f_to_remove[:i] + f_to_remove[i+1:]\n","    \n","    # Refit the model using reduced features\n","    pipe1.fit(input_train[reduced_features], output_train)\n","    y_hat_reduced = pipe1.predict(input_validation[reduced_features])\n","    \n","    # Calculate RMSE for the reduced model\n","    rmse_reduced = calculate_rmse(output_validation, y_hat_reduced)\n","    \n","    print(f\"RMSE after removing {f_to_remove[i]}: {rmse_reduced}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import root_mean_squared_error, mean_squared_error\n","from sklearn.model_selection import GroupKFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","import math\n","\n","# Remove y3 and x3\n","\n","model = LinearRegression()\n","pipe1 = Pipeline(\n","    [\n","            ('scaling', StandardScaler()), \n","            (\"model\", model)\n","    ])\n","\n","reduced_input=input_train[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\"]]\n","reduced_val=input_validation[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\"]]\n","\n","# Function to calculate RMSE\n","def calculate_rmse(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred, squared=False)\n","    return math.sqrt(mse)\n","\n","# Initialize baseline RMSE\n","pipe1.fit(reduced_input, output_train)\n","y_hat1 = pipe1.predict(reduced_val)\n","baseline_rmse = calculate_rmse(output_validation, y_hat1)\n","\n","print(\"After removing y3 and x3: \", baseline_rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(y_hat1).to_numpy())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MinMaxScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","pipe3 = Pipeline(\n","    [('scaling', MinMaxScaler()), \n","    (\"model\", LinearRegression())\n","\n","    ]\n",")\n","pipe3.fit(input_train[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]], output_train)\n","y_hat3 = pipe3.predict(input_validation[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]])\n","mse3 = mean_squared_error(output_validation, y_hat3, squared = False)\n","\n","print(\"RMSE: \", math.sqrt(mse3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# QuantileTransformer\n","from sklearn.preprocessing import QuantileTransformer\n","\n","pipe4 = Pipeline(\n","    [('scaling', QuantileTransformer()), \n","    (\"model\", LinearRegression())\n","\n","    ]\n",")\n","pipe4.fit(input_train[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]], output_train)\n","y_hat4 = pipe4.predict(input_validation[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]])\n","mse4 = mean_squared_error(output_validation, y_hat4, squared = False)\n","\n","print(\"RMSE: \", math.sqrt(mse4))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# RobustScaler\n","from sklearn.preprocessing import RobustScaler\n","\n","pipe5 = Pipeline(\n","    [('scaling', RobustScaler()), \n","    (\"model\", LinearRegression())\n","\n","    ]\n",")\n","pipe5.fit(input_train[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]], output_train)\n","y_hat5 = pipe5.predict(input_validation[[\"t\",\"x_1_init\",\"y_1_init\",\"x_2_init\", \"y_2_init\",\"x_3_init\",\"y_3_init\"]])\n","mse5 = mean_squared_error(output_validation, y_hat5, squared = False)\n","\n","print(\"RMSE: \", math.sqrt(mse5))"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3.2 Evaluation of Variable Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe_best_degree= Pipeline(\n","[       ('preprocessor', PolynomialFeatures(degree=3)),\n","        ('model', RidgeCV(alphas=np.logspace(-6, 6, 13)))\n","])\n","\n","input_train_remove_var = input_train.copy()\n","input_train_remove_var = input_train_remove_var.drop(columns=[\"x_3_init\", \"y_3_init\"])\n","\n","input_validation_remove_var = input_validation.copy()\n","input_validation_remove_var = input_validation_remove_var.drop(columns=[\"x_3_init\", \"y_3_init\"])\n","\n","pipe_best_degree.fit(input_train_remove_var, output_train)\n","\n","output_train_predict = pipe_best_degree.predict(input_train_remove_var)\n","output_val_predict = pipe_best_degree.predict(input_validation_remove_var)\n","\n","train_error = root_mean_squared_error(output_train, output_train_predict)\n","print(\"Train RMSE:\", train_error)\n","\n","val_error = root_mean_squared_error(output_validation, output_val_predict)\n","print(\"Validation RMSE:\", val_error)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(output_val_predict).to_numpy(), \"3_2\")"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3.3 Adding Variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_train_add_var = input_train.copy()\n","input_train_add_var[\"d12\"] = np.sqrt((input_train[\"x_1_init\"] - input_train[\"x_2_init\"])**2 + \n","                                     (input_train[\"y_1_init\"] - input_train[\"y_2_init\"])**2)\n","\n","input_train_add_var[\"d13\"] = np.sqrt((input_train[\"x_1_init\"] - input_train[\"x_3_init\"])**2 + \n","                                     (input_train[\"y_1_init\"] - input_train[\"y_3_init\"])**2)\n","\n","input_train_add_var[\"d23\"] = np.sqrt((input_train[\"x_3_init\"] - input_train[\"x_2_init\"])**2 + \n","                                     (input_train[\"y_3_init\"] - input_train[\"y_2_init\"])**2)\n","\n","input_validation_add_var = input_validation.copy()\n","input_validation_add_var[\"d12\"] = np.sqrt((input_validation[\"x_1_init\"] - input_validation[\"x_2_init\"])**2 + \n","                                     (input_validation[\"y_1_init\"] - input_validation[\"y_2_init\"])**2)\n","\n","input_validation_add_var[\"d13\"] = np.sqrt((input_validation[\"x_1_init\"] - input_validation[\"x_3_init\"])**2 + \n","                                     (input_validation[\"y_1_init\"] - input_validation[\"y_3_init\"])**2)\n","\n","input_validation_add_var[\"d23\"] = np.sqrt((input_validation[\"x_3_init\"] - input_validation[\"x_2_init\"])**2 + \n","                                     (input_validation[\"y_3_init\"] - input_validation[\"y_2_init\"])**2)\n","\n","input_train_add_var.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train_add_var, output_train)\n","y_hat = pipe.predict(input_validation_add_var)\n","rmse_distance = root_mean_squared_error(output_validation, y_hat)\n","\n","print(\"RMSE: \", rmse_distance)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Creating a copy of the DataFrame\n","input_train_add_var_1 = input_train.copy()\n","\n","# Calculating the x12 ratio with a check for division by zero\n","input_train_add_var_1[\"x12_ratio\"] = np.where(input_train[\"x_2_init\"] != 0, \n","                                              input_train[\"x_1_init\"] / input_train[\"x_2_init\"], \n","                                              0)\n","\n","# Calculating the x13 ratio with a check for division by zero\n","input_train_add_var_1[\"x13_ratio\"] = np.where(input_train[\"x_3_init\"] != 0, \n","                                              input_train[\"x_1_init\"] / input_train[\"x_3_init\"], \n","                                              0)\n","\n","# Calculating the x23 ratio with a check for division by zero\n","input_train_add_var_1[\"x23_ratio\"] = np.where(input_train[\"x_3_init\"] != 0, \n","                                              input_train[\"x_2_init\"] / input_train[\"x_3_init\"], \n","                                              0)\n","\n","# Creating a copy of the DataFrame\n","input_val = input_validation.copy()\n","\n","# Calculating the x12 ratio with a check for division by zero\n","input_val[\"x12_ratio\"] = np.where(input_validation[\"x_2_init\"] != 0, \n","                                              input_validation[\"x_1_init\"] / input_validation[\"x_2_init\"], \n","                                              0)\n","\n","# Calculating the x13 ratio with a check for division by zero\n","input_val[\"x13_ratio\"] = np.where(input_validation[\"x_3_init\"] != 0, \n","                                              input_validation[\"x_1_init\"] / input_validation[\"x_3_init\"], \n","                                              0)\n","\n","# Calculating the x23 ratio with a check for division by zero\n","input_val[\"x23_ratio\"] = np.where(input_validation[\"x_3_init\"] != 0, \n","                                              input_validation[\"x_2_init\"] / input_validation[\"x_3_init\"], \n","                                              0)\n","\n","input_val.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train_add_var_1, output_train)\n","y_hat = pipe.predict(input_val)\n","rmse_ratiox = root_mean_squared_error(output_validation, y_hat)\n","\n","print(\"RMSE: \", rmse_ratiox)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Creating a copy of the DataFrame\n","input_train_add_var_1 = input_train.copy()\n","\n","# Calculating the x12 ratio with a check for division by zero\n","input_train_add_var_1[\"x1_y1_ratio\"] = np.where(input_train[\"y_1_init\"] != 0, \n","                                              input_train[\"x_1_init\"] / input_train[\"y_1_init\"], \n","                                              0)\n","\n","# Calculating the x13 ratio with a check for division by zero\n","input_train_add_var_1[\"x2_y2_ratio\"] = np.where(input_train[\"y_2_init\"] != 0, \n","                                              input_train[\"x_2_init\"] / input_train[\"y_2_init\"], \n","                                              0)\n","\n","# Calculating the x23 ratio with a check for division by zero\n","input_train_add_var_1[\"x3_y3_ratio\"] = np.where(input_train[\"y_3_init\"] != 0, \n","                                              input_train[\"x_3_init\"] / input_train[\"y_3_init\"], \n","                                              0)\n","\n","# Creating a copy of the DataFrame\n","input_val = input_validation.copy()\n","\n","# Calculating the x12 ratio with a check for division by zero\n","input_val[\"x1_y1_ratio\"] = np.where(input_validation[\"y_1_init\"] != 0, \n","                                              input_validation[\"x_1_init\"] / input_validation[\"y_1_init\"], \n","                                              0)\n","\n","# Calculating the x13 ratio with a check for division by zero\n","input_val[\"x2_y2_ratio\"] = np.where(input_validation[\"y_2_init\"] != 0, \n","                                              input_validation[\"x_2_init\"] / input_validation[\"y_2_init\"], \n","                                              0)\n","\n","# Calculating the x23 ratio with a check for division by zero\n","input_val[\"x3_y3_ratio\"] = np.where(input_validation[\"y_3_init\"] != 0, \n","                                              input_validation[\"x_3_init\"] / input_validation[\"y_3_init\"], \n","                                              0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_val.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train_add_var_1, output_train)\n","y_hat = pipe.predict(input_val)\n","rmse_ratio_points = root_mean_squared_error(output_validation, y_hat)\n","\n","print(\"RMSE: \", rmse_ratio_points)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Creating a copy of the train DataFrame\n","input_train_add_var_1 = input_train.copy()\n","\n","# Calculating the inverse of x1, x2, x3 with a check for division by zero (train data)\n","input_train_add_var_1[\"x1_inverse\"] = np.where(input_train[\"x_1_init\"] != 0, 1 / input_train[\"x_1_init\"], 0)\n","input_train_add_var_1[\"x2_inverse\"] = np.where(input_train[\"x_2_init\"] != 0, 1 / input_train[\"x_2_init\"], 0)\n","input_train_add_var_1[\"x3_inverse\"] = np.where(input_train[\"x_3_init\"] != 0, 1 / input_train[\"x_3_init\"], 0)\n","\n","# Creating a copy of the validation DataFrame\n","input_val = input_validation.copy()\n","\n","# Calculating the inverse of x1, x2, x3 with a check for division by zero (validation data)\n","input_val[\"x1_inverse\"] = np.where(input_validation[\"x_1_init\"] != 0, 1 / input_validation[\"x_1_init\"], 0)\n","input_val[\"x2_inverse\"] = np.where(input_validation[\"x_2_init\"] != 0, 1 / input_validation[\"x_2_init\"], 0)\n","input_val[\"x3_inverse\"] = np.where(input_validation[\"x_3_init\"] != 0, 1 / input_validation[\"x_3_init\"], 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train_add_var_1, output_train)\n","y_hat = pipe.predict(input_val)\n","rmse_inverse = root_mean_squared_error(output_validation, y_hat)\n","\n","print(\"RMSE: \", rmse_inverse)"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Creating a copy of the train DataFrame\n","input_train_add_var_1 = input_train.copy()\n","\n","# Gravitational constant (G)\n","G = 6.67430e-11  # You can adjust this value as needed\n","\n","# Calculating the x1_g term with division by zero protection (train data)\n","input_train_add_var_1[\"x1_g\"] = np.where((input_train[\"x_1_init\"] - input_train[\"x_2_init\"] != 0) & \n","                                         (input_train[\"x_1_init\"] - input_train[\"x_3_init\"] != 0),\n","                                         -G * np.where(input_train[\"x_1_init\"] - input_train[\"x_2_init\"] != 0,\n","                                                       (input_train[\"x_1_init\"] - input_train[\"x_2_init\"]) / (abs(input_train[\"x_1_init\"] - input_train[\"x_2_init\"])**3), 0)\n","                                         -G * np.where(input_train[\"x_1_init\"] - input_train[\"x_3_init\"] != 0,\n","                                                       (input_train[\"x_1_init\"] - input_train[\"x_3_init\"]) / (abs(input_train[\"x_1_init\"] - input_train[\"x_3_init\"])**3), 0),\n","                                         0)\n","\n","# Calculating the x2_g term with division by zero protection\n","input_train_add_var_1[\"x2_g\"] = -G * np.where(input_train[\"x_2_init\"] - input_train[\"x_3_init\"] != 0,\n","                                              (input_train[\"x_2_init\"] - input_train[\"x_3_init\"]) / (abs(input_train[\"x_2_init\"] - input_train[\"x_3_init\"])**3), 0) \\\n","                                -G * np.where(input_train[\"x_2_init\"] - input_train[\"x_1_init\"] != 0,\n","                                              (input_train[\"x_2_init\"] - input_train[\"x_1_init\"]) / (abs(input_train[\"x_2_init\"] - input_train[\"x_1_init\"])**3), 0)\n","\n","# Calculating the x3_g term with division by zero protection\n","input_train_add_var_1[\"x3_g\"] = -G * np.where(input_train[\"x_3_init\"] - input_train[\"x_1_init\"] != 0,\n","                                              (input_train[\"x_3_init\"] - input_train[\"x_1_init\"]) / (abs(input_train[\"x_3_init\"] - input_train[\"x_1_init\"])**3), 0) \\\n","                                -G * np.where(input_train[\"x_3_init\"] - input_train[\"x_2_init\"] != 0,\n","                                              (input_train[\"x_3_init\"] - input_train[\"x_2_init\"]) / (abs(input_train[\"x_3_init\"] - input_train[\"x_2_init\"])**3), 0)\n","\n","# Calculating the y1_g term with division by zero protection\n","input_train_add_var_1[\"y1_g\"] = -G * np.where(input_train[\"y_1_init\"] - input_train[\"y_2_init\"] != 0,\n","                                              (input_train[\"y_1_init\"] - input_train[\"y_2_init\"]) / (abs(input_train[\"y_1_init\"] - input_train[\"y_2_init\"])**3), 0) \\\n","                                -G * np.where(input_train[\"y_1_init\"] - input_train[\"y_3_init\"] != 0,\n","                                              (input_train[\"y_1_init\"] - input_train[\"y_3_init\"]) / (abs(input_train[\"y_1_init\"] - input_train[\"y_3_init\"])**3), 0)\n","\n","# Calculating the y2_g term with division by zero protection\n","input_train_add_var_1[\"y2_g\"] = -G * np.where(input_train[\"y_2_init\"] - input_train[\"y_3_init\"] != 0,\n","                                              (input_train[\"y_2_init\"] - input_train[\"y_3_init\"]) / (abs(input_train[\"y_2_init\"] - input_train[\"y_3_init\"])**3), 0) \\\n","                                -G * np.where(input_train[\"y_2_init\"] - input_train[\"y_1_init\"] != 0,\n","                                              (input_train[\"y_2_init\"] - input_train[\"y_1_init\"]) / (abs(input_train[\"y_2_init\"] - input_train[\"y_1_init\"])**3), 0)\n","\n","# Calculating the y3_g term with division by zero protection\n","input_train_add_var_1[\"y3_g\"] = -G * np.where(input_train[\"y_3_init\"] - input_train[\"y_1_init\"] != 0,\n","                                              (input_train[\"y_3_init\"] - input_train[\"y_1_init\"]) / (abs(input_train[\"y_3_init\"] - input_train[\"y_1_init\"])**3), 0) \\\n","                                -G * np.where(input_train[\"y_3_init\"] - input_train[\"y_2_init\"] != 0,\n","                                              (input_train[\"y_3_init\"] - input_train[\"y_2_init\"]) / (abs(input_train[\"y_3_init\"] - input_train[\"y_2_init\"])**3), 0)\n","\n","# Now repeat the same process for the validation dataset\n","\n","# Creating a copy of the validation DataFrame\n","input_val = input_validation.copy()\n","\n","# Calculating the x1_g term with division by zero protection (validation data)\n","input_val[\"x1_g\"] = np.where((input_validation[\"x_1_init\"] - input_validation[\"x_2_init\"] != 0) & \n","                             (input_validation[\"x_1_init\"] - input_validation[\"x_3_init\"] != 0),\n","                             -G * np.where(input_validation[\"x_1_init\"] - input_validation[\"x_2_init\"] != 0,\n","                                           (input_validation[\"x_1_init\"] - input_validation[\"x_2_init\"]) / (abs(input_validation[\"x_1_init\"] - input_validation[\"x_2_init\"])**3), 0)\n","                             -G * np.where(input_validation[\"x_1_init\"] - input_validation[\"x_3_init\"] != 0,\n","                                           (input_validation[\"x_1_init\"] - input_validation[\"x_3_init\"]) / (abs(input_validation[\"x_1_init\"] - input_validation[\"x_3_init\"])**3), 0),\n","                             0)\n","\n","# Calculating the x2_g term with division by zero protection\n","input_val[\"x2_g\"] = -G * np.where(input_validation[\"x_2_init\"] - input_validation[\"x_3_init\"] != 0,\n","                                  (input_validation[\"x_2_init\"] - input_validation[\"x_3_init\"]) / (abs(input_validation[\"x_2_init\"] - input_validation[\"x_3_init\"])**3), 0) \\\n","                    -G * np.where(input_validation[\"x_2_init\"] - input_validation[\"x_1_init\"] != 0,\n","                                  (input_validation[\"x_2_init\"] - input_validation[\"x_1_init\"]) / (abs(input_validation[\"x_2_init\"] - input_validation[\"x_1_init\"])**3), 0)\n","\n","# Calculating the x3_g term with division by zero protection\n","input_val[\"x3_g\"] = -G * np.where(input_validation[\"x_3_init\"] - input_validation[\"x_1_init\"] != 0,\n","                                  (input_validation[\"x_3_init\"] - input_validation[\"x_1_init\"]) / (abs(input_validation[\"x_3_init\"] - input_validation[\"x_1_init\"])**3), 0) \\\n","                    -G * np.where(input_validation[\"x_3_init\"] - input_validation[\"x_2_init\"] != 0,\n","                                  (input_validation[\"x_3_init\"] - input_validation[\"x_2_init\"]) / (abs(input_validation[\"x_3_init\"] - input_validation[\"x_2_init\"])**3), 0)\n","\n","# Calculating the y1_g term with division by zero protection\n","input_val[\"y1_g\"] = -G * np.where(input_validation[\"y_1_init\"] - input_validation[\"y_2_init\"] != 0,\n","                                  (input_validation[\"y_1_init\"] - input_validation[\"y_2_init\"]) / (abs(input_validation[\"y_1_init\"] - input_validation[\"y_2_init\"])**3), 0) \\\n","                    -G * np.where(input_validation[\"y_1_init\"] - input_validation[\"y_3_init\"] != 0,\n","                                  (input_validation[\"y_1_init\"] - input_validation[\"y_3_init\"]) / (abs(input_validation[\"y_1_init\"] - input_validation[\"y_3_init\"])**3), 0)\n","\n","# Calculating the y2_g term with division by zero protection\n","input_val[\"y2_g\"] = -G * np.where(input_validation[\"y_2_init\"] - input_validation[\"y_3_init\"] != 0,\n","                                  (input_validation[\"y_2_init\"] - input_validation[\"y_3_init\"]) / (abs(input_validation[\"y_2_init\"] - input_validation[\"y_3_init\"])**3), 0) \\\n","                    -G * np.where(input_validation[\"y_2_init\"] - input_validation[\"y_1_init\"] != 0,\n","                                  (input_validation[\"y_2_init\"] - input_validation[\"y_1_init\"]) / (abs(input_validation[\"y_2_init\"] - input_validation[\"y_1_init\"])**3), 0)\n","\n","# Calculating the y3_g term with division by zero protection\n","input_val[\"y3_g\"] = -G * np.where(input_validation[\"y_3_init\"] - input_validation[\"y_1_init\"] != 0,\n","                                  (input_validation[\"y_3_init\"] - input_validation[\"y_1_init\"]) / (abs(input_validation[\"y_3_init\"] - input_validation[\"y_1_init\"])**3), 0) \\\n","                    -G * np.where(input_validation[\"y_3_init\"] - input_validation[\"y_2_init\"] != 0,\n","                                  (input_validation[\"y_3_init\"] - input_validation[\"y_2_init\"]) / (abs(input_validation[\"y_3_init\"] - input_validation[\"y_2_init\"])**3), 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train_add_var_1, output_train)\n","y_hat = pipe.predict(input_val)\n","rmse_g = root_mean_squared_error(output_validation, y_hat)\n","\n","print(\"RMSE: \", rmse_g)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Sample RMSE values for each strategy\n","rmses = {\n","    \"Baseline\": rmse,\n","    \"Distances\": rmse_distance,\n","    \"Ratio_coeff\": rmse_ratiox,\n","    \"Ratio_body\": rmse_ratio_points,\n","    \"Inverses\": rmse_inverse,\n","    \"Forces\": rmse_g\n","}\n","\n","# Plotting the RMSEs with a customized y-axis range for better visibility\n","plt.figure(figsize=(10, 6))\n","plt.bar(rmses.keys(), rmses.values(), color='skyblue')\n","plt.title('RMSE Comparison of Different Feature Engineering Strategies', fontsize=14)\n","plt.xlabel('Strategy', fontsize=12)\n","plt.ylabel('RMSE', fontsize=12)\n","plt.xticks(rotation=45)\n","\n","# Adjusting y-axis to focus on smaller differences (e.g., between 0.07 and 0.15)\n","plt.ylim(min(rmses.values()) - 0.01, max(rmses.values()) + 0.01)\n","\n","plt.tight_layout()\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3.4 Evaluation of Variable Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","\n","def create_custom_features(X):\n","    X_new = X.copy()\n","    X_new[\"d12\"] = np.sqrt((X[\"x_1_init\"] - X[\"x_2_init\"])**2 + \n","                                     (X[\"y_1_init\"] - X[\"y_2_init\"])**2)\n","\n","    X_new[\"d13\"] = np.sqrt((X[\"x_1_init\"] - X[\"x_3_init\"])**2 + \n","                                        (X[\"y_1_init\"] - X[\"y_3_init\"])**2)\n","\n","    X_new[\"d23\"] = np.sqrt((X[\"x_3_init\"] - X[\"x_2_init\"])**2 + \n","                                        (X[\"y_3_init\"] - X[\"y_2_init\"])**2)\n","    return X_new\n","\n","pipe_best_degree = Pipeline([\n","            # Step 1: Create custom features like ratios\n","            ('custom_features', FunctionTransformer(create_custom_features, validate=False)),\n","            \n","            # Step 2: Apply column-wise transformation for polynomial and scaling\n","            ('preprocessor', ColumnTransformer(\n","                transformers=[\n","                    # Apply PolynomialFeatures to the original features\n","                    ('poly', PolynomialFeatures(degree=3, include_bias=False), ['x_1_init', 'x_2_init', 'x_3_init']),\n","                    \n","                    # Apply scaling to custom features created in Step 1\n","                    ('scaler', StandardScaler(), ['d12', 'd13', 'd23'])\n","                ],\n","                remainder='passthrough'  # To pass through any unlisted columns\n","            )),\n","            \n","            # Step 3: RidgeCV model with alpha selection\n","            ('model', RidgeCV(alphas=np.logspace(-6, 6, 13)))  # RidgeCV with cross-validated alpha\n","        ])\n","\n","pipe_best_degree.fit(input_train, output_train)\n","\n","output_train_predict = pipe_best_degree.predict(input_train)\n","output_val_predict = pipe_best_degree.predict(input_validation)\n","output_submission_predict = pipe_best_degree.predict(input_submission)\n","\n","train_error = np.sqrt(mean_squared_error(output_train, output_train_predict))\n","print(\"Train RMSE:\", train_error)\n","\n","val_error = np.sqrt(mean_squared_error(output_validation, output_val_predict))\n","print(\"Validation RMSE:\", val_error)\n","\n","save_predictions_to_csv(output_submission_predict, \"augmented_polynomial_submission\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(output_val_predict).to_numpy(), \"3_4\")"]},{"cell_type":"markdown","metadata":{},"source":["# Task 4: Nonparamentric Model â€” the k-Nearest Neighbors Regressor\n","## Task 4.1 Development"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["    \n","from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import root_mean_squared_error\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["analytics = []\n","def validate_knn_regression(X_train, y_train, X_val, y_val, k=range(1,15)):\n","    for n_neighbors in k:\n","        pipe = Pipeline(\n","            [\n","                    ('scaling', StandardScaler()), \n","                    (\"model\", KNeighborsRegressor(n_neighbors=n_neighbors))\n","            ])\n","        start_time = time.time()\n","\n","        pipe.fit(X_train, y_train)\n","        y_hat = pipe.predict(X_val)\n","        elapsed_time = time.time() - start_time\n","        rmse = root_mean_squared_error(y_hat, y_val)\n","\n","        print(f'k: {n_neighbors} with RMSE: {rmse}, time: {elapsed_time}')\n","        analytics.append([n_neighbors, rmse, elapsed_time])\n","\n","validate_knn_regression(input_train, output_train, input_validation, output_validation, k=range(1,15))\n","#validate_knn_regression(input_train_add_var, output_train, input_validation_add_var, output_validation, k=range(1,15)) #3.3 best \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract data for plotting\n","k_values = [item[0] for item in analytics]\n","rmse_values = [item[1] for item in analytics]\n","time_values = [item[2] for item in analytics]\n","\n","# Create a figure with subplots\n","plt.figure(figsize=(12, 6))\n","\n","# Plot elapsed time vs. k-neighbors\n","plt.subplot(1, 2, 1)\n","plt.plot(k_values, time_values, marker='o', linestyle='-', color='r', label='Elapsed Time')\n","plt.title('Elapsed Time vs. k-neighbors')\n","plt.xlabel('Number of Neighbors (k)')\n","plt.ylabel('Elapsed Time (seconds)')\n","plt.grid(True)\n","plt.legend()\n","\n","# Plot RMSE vs. k-neighbors\n","plt.subplot(1, 2, 2)\n","plt.plot(k_values, rmse_values, marker='o', linestyle='-', color='b', label='RMSE')\n","plt.title('RMSE vs. k-neighbors')\n","plt.xlabel('Number of Neighbors (k)')\n","plt.ylabel('Root Mean Squared Error (RMSE)')\n","plt.grid(True)\n","plt.legend()\n","plt.savefig(\"./output/plots/knn.pdf\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe = Pipeline(\n","    [\n","            ('scaling', StandardScaler()), \n","            (\"model\", KNeighborsRegressor(n_neighbors=13, weights='distance',\n","                                                  algorithm='brute', n_jobs=-1))\n","    ])\n","\n","pipe.fit(input_train, output_train)\n","print('trained')\n","y_hat = pipe.predict(input_validation)\n","y_hat_test = pipe.predict(input_test)\n","y_hat_submission = pipe.predict(input_submission)\n","save_predictions_to_csv(y_hat_test, \"knn_test\")\n","save_predictions_to_csv(y_hat_submission, \"knn_submission\")\n","rmse = root_mean_squared_error(output_validation, y_hat)\n","rmse_test = root_mean_squared_error(output_test, y_hat_test)\n","\n","print(\"RMSE: \", rmse)\n","print(\"RMSE Test: \", rmse_test)\n","print(f'k: {13} with RMSE: {rmse}')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Task 4.2 Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_y_yhat(output_validation.to_numpy(), pd.DataFrame(y_hat).to_numpy(), plot_title=\"knn_yhat\")"]},{"cell_type":"markdown","metadata":{},"source":["# Task 5 [Optional]"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.1 LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import root_mean_squared_error\n","from sklearn.multioutput import MultiOutputRegressor\n","import lightgbm as lgb\n","\n","pipe = Pipeline(\n","    [\n","        ('scaling', StandardScaler()), \n","        ('model', MultiOutputRegressor(\n","            lgb.LGBMRegressor(num_leaves=100, learning_rate=0.01, n_estimators=500, num_iterations=1000)\n","        ))\n","    ]\n",")\n","\n","pipe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","pipe.fit(input_train, output_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_hat = pipe.predict(input_validation)\n","y_hat_test = pipe.predict(input_test)\n","y_hat_submission = pipe.predict(input_submission)\n","save_predictions_to_csv(y_hat_test, \"lgbm_test\")\n","save_predictions_to_csv(y_hat_submission, \"lgbm_submission\")\n","rmse = root_mean_squared_error(output_validation, y_hat)\n","rmse_test = root_mean_squared_error(output_test, y_hat_test)\n","\n","print(\"RMSE: \", rmse)\n","print(\"RMSE Test: \", rmse_test)\n","print(f'LightGBM with RMSE: {rmse}')"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.2 Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import root_mean_squared_error\n","from sklearn.neural_network import MLPRegressor\n","\n","pipe = Pipeline(\n","    [\n","        ('scaling', StandardScaler()), \n","        ('model', MLPRegressor(\n","            alpha=0.01,\n","            solver='adam',\n","            random_state=1, \n","            max_iter=400,\n","            activation='relu',\n","            hidden_layer_sizes=(100, 100, 500, 500, 100, 100, 100,100, 100)\n","        ))\n","    ]\n",")\n","\n","pipe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipe.fit(input_train, output_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_hat = pipe.predict(input_validation)\n","y_hat_test = pipe.predict(input_test)\n","y_hat_submission = pipe.predict(input_submission)\n","save_predictions_to_csv(y_hat_test, \"nn_test\")\n","save_predictions_to_csv(y_hat_submission, \"nn_submission\")\n","rmse = root_mean_squared_error(output_validation, y_hat)\n","rmse_test = root_mean_squared_error(output_test, y_hat_test)\n","\n","print(\"RMSE: \", rmse)\n","print(\"RMSE Test: \", rmse_test)\n","print(f'NeuralNetworks with RMSE: {rmse}')"]},{"cell_type":"markdown","metadata":{},"source":["#### AutoML"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import autosklearn\n","\n","model = autosklearn.regression.AutoSklearnRegressor(\n","    memory_limit=80048,\n","    time_left_for_this_task=60*30,\n","    per_run_time_limit=60*30/10,\n","    tmp_folder=\"./tmp/autosklearn_regression_example_tmp\",\n","    n_jobs=-1,\n","    ensemble_size=0,\n","    metric=autosklearn.metrics.mean_squared_error\n",")\n","model.fit(input_train, output_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.leaderboard()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_hat = model.predict(input_validation)\n","y_hat_test = model.predict(input_test)\n","y_hat_submission = model.predict(input_submission)\n","save_predictions_to_csv(y_hat_test, \"automl_test\")\n","save_predictions_to_csv(y_hat_submission, \"automl_submission\")\n","rmse = root_mean_squared_error(output_validation, y_hat)\n","rmse_test = root_mean_squared_error(output_test, y_hat_test)\n","print(\"RMSE: \", rmse)\n","print(\"RMSE Test: \", rmse_test)\n","print(f'Automl with RMSE: {rmse}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5811072,"sourceId":9539706,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":4}
